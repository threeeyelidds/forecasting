{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune with Lora: https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md\n",
    "# Training script: https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh\n",
    "\n",
    "# generate training data\n",
    "\n",
    "# [\n",
    "#   {\n",
    "#     \"id\": \"997bb945-628d-4724-b370-b84de974a19f\",\n",
    "#     \"image\": \"part-000001/997bb945-628d-4724-b370-b84de974a19f.jpg\",\n",
    "#     \"conversations\": [\n",
    "#       {\n",
    "#         \"from\": \"human\",\n",
    "#         \"value\": \"<image>\\nWrite a prompt for Stable Diffusion to generate this image.\"\n",
    "#       },\n",
    "#       {\n",
    "#         \"from\": \"gpt\",\n",
    "#         \"value\": \"a beautiful painting of chernobyl by nekro, pascal blanche, john harris, greg rutkowski, sin jong hun, moebius, simon stalenhag. in style of cg art. ray tracing. cel shading. hyper detailed. realistic. ue 5. maya. octane render. \"\n",
    "#       },\n",
    "#     ]\n",
    "#   },\n",
    "#   ...\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "annotation_dir = Path(\"/mnt/vol_c/ego4d_data/v1/annotations\")\n",
    "object_detection_path = Path(\"/mnt/vol_c/ego4d_data/v1/sta_models/object_detections.json\")\n",
    "\n",
    "def gen_conversation(annotation_path: Path, image_dir: Path, object_detection_path: Path=None) -> None:\n",
    "    with open(annotation_path, \"r\") as f:\n",
    "        # ['info', 'annotations', 'noun_categories', 'verb_categories']\n",
    "        annotation = json.load(f)\n",
    "    # with open(object_detection_path, \"r\") as f:\n",
    "    #     object_detection = json.load(f)\n",
    "    # print(object_detection.keys())\n",
    "\n",
    "    conversation = []\n",
    "    noun_categories = {\n",
    "        pair[\"id\"]: pair[\"name\"] for pair in annotation[\"noun_categories\"]\n",
    "    }\n",
    "    verb_categories = {\n",
    "        pair[\"id\"]: pair[\"name\"] for pair in annotation[\"verb_categories\"]\n",
    "    }\n",
    "    noun_categories = {\n",
    "        k: v.split(\"_\")[0] for k, v in noun_categories.items()\n",
    "    }  # naive way to get the name (use the first noun before \"_\")\n",
    "    verb_categories = {\n",
    "        k: v.split(\"_\")[0] for k, v in verb_categories.items()\n",
    "    }  # naive way to get the name (use the first verb before \"_\")\n",
    "    annotations = annotation[\"annotations\"]\n",
    "    ONE = \"one\"\n",
    "    {\n",
    "        \"uid\": \"cde41c4f-50d1-4910-9f2a-4c7b6987df92_0000468\",\n",
    "        \"video_id\": \"cde41c4f-50d1-4910-9f2a-4c7b6987df92\",\n",
    "        \"frame\": 468,\n",
    "        \"clip_id\": 8,\n",
    "        \"clip_uid\": \"8d686451-cac9-4526-a022-b6eaf7d467b4\",\n",
    "        \"clip_frame\": 468,\n",
    "        \"objects\": [\n",
    "            {\n",
    "                \"box\": [938.09, 1350.28, 1138.58, 1441.4099999999999],\n",
    "                \"verb_category_id\": 62,\n",
    "                \"noun_category_id\": 66,\n",
    "                \"time_to_contact\": 0.9666666666666667,\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    # p73 metric def\n",
    "    for ann in annotations:\n",
    "        gpt_value_str = json.dumps({\n",
    "                \"objName\": noun_categories[ann['objects'][0]['noun_category_id']],\n",
    "                \"actType\": verb_categories[ann['objects'][0]['verb_category_id']],\n",
    "                \"timeUntilContact\": ann['objects'][0]['time_to_contact']\n",
    "            })\n",
    "        conversation.append(\n",
    "            {\n",
    "                \"id\": ann[\"uid\"],\n",
    "                \"image\": f\"{image_dir}/{ann['uid']}.jpg\",\n",
    "                # \"image\": f\"forecasting/short_term_anticipation/data/films/{ann['video_id']}/{ann['uid']}.png\",\n",
    "                \"conversations\": [\n",
    "                    {\n",
    "                        \"from\": \"human\",\n",
    "                        \"value\":\n",
    "                            \"<image>\\n\"\n",
    "                            \"Anticipate the next object the hand will interact with, detailing the object's class, the verb describing the interaction, and the estimated time to contact. \" # task\n",
    "                            \"Provide your response in the format 'n: <noun_category>, v: <verb_category>, bbox: [x_min, y_min, x_max, y_max], t: <time_to_contact>',\\n\" # format\n",
    "                            f\"where <noun_category> should be one of the noun in {list(noun_categories.values())},\\n\" # noun\n",
    "                            f\"<verb_category> should be one of the verb in {list(verb_categories.values())} and,\\n\" # verb\n",
    "                            \"<time_to_contact> is a float in second.\", # ttc\n",
    "                            # \"This is a picture from a video taken from the view of a person performing some action. \"\n",
    "                            # \"Please identify the objects in this image that the person is likely to interact with. For each object, \"\n",
    "                            # \"predict three things: a noun, a verb, and a number in seconds. \"\n",
    "                            # \"The noun is the object that the person is going to interact with.\"\n",
    "                            # \"The verb describes how the person will interact with the object. \"\n",
    "                            # \"The number describes, in seconds, when the person will interact with the object. \"\n",
    "                            # f\"Please predict {ONE} objects, and prioritize objects the person is immediately interacting with. \"\n",
    "                            # f\"Please respond in JSON format, which contains a list of \"\n",
    "                            # \"JSON dictionaries with fields 'objName', 'objDesc', 'actType', and 'timeUntilContact'. \"\n",
    "                            # \"Some examples of such a JSON dictionary are: \"\n",
    "                            # \"{'objName': 'mower', 'actType': 'put', 'timeUntilContact': 1.2}, \"\n",
    "                            # \"{'objName': 'stone', 'actType': 'take', 'timeUntilContact': 0.5}, \"\n",
    "                            # \"{'objName': 'pot', 'actType': 'clean', 'timeUntilContact': 0.4}, \"\n",
    "                            # \"{'objName': 'dumbbell', 'actType': 'take', 'timeUntilContact': 1.3}, \"\n",
    "                            # \"{'objName': 'scissors', 'actType': 'move', 'timeUntilContact': 0.2}.\"\n",
    "                            # f\"Please make sure that actionType, the verb, is one of these: {list(verb_categories.values())}. \"\n",
    "                            # f\"Please make sure that objName, the noun, is one of these: {list(noun_categories.values())}. \"\n",
    "                    },\n",
    "                    {\n",
    "                        \"from\": \"gpt\",\n",
    "                        \"value\": # gpt_value_str\n",
    "                            f\"n: {noun_categories[ann['objects'][0]['noun_category_id']]}, \"\n",
    "                            f\"v: {verb_categories[ann['objects'][0]['verb_category_id']]}, \"\n",
    "                            f\"bbox: {ann['objects'][0]['box']}, \"\n",
    "                            f\"t: {ann['objects'][0]['time_to_contact']}\",\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "            \n",
    "        )\n",
    "\n",
    "    return conversation\n",
    "\n",
    "conversation_json = gen_conversation(annotation_dir / \"fho_sta_train.json\", image_dir=\"IMAGE_DIR\")\n",
    "output_json_path = Path(\"/mnt/vol_c/forecasting/short_term_anticipation/prompts/prompts_v2.json\")\n",
    "# dump conversation_json to output_json_path\n",
    "with open(output_json_path, \"w\") as f:\n",
    "    json.dump(conversation_json, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ego4d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
